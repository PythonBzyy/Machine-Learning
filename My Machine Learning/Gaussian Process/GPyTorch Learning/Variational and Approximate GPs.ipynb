{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e985d894aa59910e",
   "metadata": {},
   "source": [
    "# Variational and Approximate GPs \n",
    "\n",
    "变分和近似高斯过程用于各种情况:\n",
    "- 当GP似然是非高斯时 (例如用于分类)。\n",
    "- 扩大GP回归 (通过使用随机优化)。\n",
    "- 将GPs作为更大概率模型的一部分。\n",
    "\n",
    "使用GPyTorch可以实现各种类型的近似GP模型。所有近似模型都由以下3个可组合对象组成:\n",
    "- `VariationalDistribution`，定义了后验近似诱导值的形式 $q(\\boldsymbol u)$\n",
    "- `VariationalStrategies`，定义如何通过 $q(\\boldsymbol u)$ 计算 $q(\\boldsymbol f_X)$\n",
    "- `_ApproximateMarginalLogLikelihood`，定义学习近似后验的目标函数 (e.g. variational ELBO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576da46639e9b04f",
   "metadata": {},
   "source": [
    "## Stochastic Variational GP Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdbaea0ca8fe11d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T11:27:56.058298Z",
     "start_time": "2024-04-07T11:27:52.127772Z"
    }
   },
   "source": [
    "import tqdm\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "938f3e029bc6619a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T11:27:57.478421Z",
     "start_time": "2024-04-07T11:27:57.003412Z"
    }
   },
   "source": [
    "import urllib.request\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from math import floor"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9edf878301e994fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T11:27:58.190828Z",
     "start_time": "2024-04-07T11:27:58.140461Z"
    }
   },
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "\n",
    "if not smoke_test and not os.path.isfile('./datasets/elevators.mat'):\n",
    "    print('Downloading \\'elevators\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://drive.google.com/uc?export=download&id=1jhWL3YUHvXIaftia4qeAyDwVxo6j1alk',\n",
    "                               './datasets/elevators.mat')  # 这里 mat 文件需要手动从链接中下载\n",
    "\n",
    "if smoke_test:  # this is for running the notebook in our testing framework\n",
    "    X, y = torch.randn(1000, 3), torch.randn(1000)\n",
    "else:\n",
    "    data = torch.Tensor(loadmat('./datasets/elevators.mat')['data'])\n",
    "    X = data[:, :-1]\n",
    "    X = X - X.min(0)[0]\n",
    "    X = 2 * (X / X.max(0)[0]) - 1\n",
    "    y = data[:, -1]\n",
    "\n",
    "train_n = int(floor(0.8 * len(X)))\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "# if torch.cuda.is_available():\n",
    "# train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()\n",
    "train_x, train_y, test_x, test_y = train_x.to(device), train_y.to(device), test_x.to(device), test_y.to(device)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "36bfb630f0e95881",
   "metadata": {},
   "source": [
    "### Creating a DataLoader\n",
    "\n",
    "下一步是创建一个 `DataLoader`，它将处理随机的小批量数据。这涉及到使用PyTorch提供的标准 `TensorDataset` 和 `DataLoader` 模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77910f2e19d44828",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T11:27:59.438541Z",
     "start_time": "2024-04-07T11:27:59.426615Z"
    }
   },
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)  # original: batch_size=1024\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8a6be365404ea3ce",
   "metadata": {},
   "source": [
    "### Creating a SVGP Model\n",
    "\n",
    "对于大多数变分/近似GP模型，需要构造以下GPyTorch对象:\n",
    "- 一个 **GP模型** (<font color=red>`gpytorch.models.ApproximateGP`</font>) - 处理基本的变分推断。\n",
    "- Variational distribution (<font color=red>`gpytorch.variational._VariationalDistribution`</font>) - 这告诉我们 $q(\\boldsymbol u)$ 的变分分布应该是什么形式。\n",
    "- Variational strategy (<font color=red> `gpytorch.variational._VariationalStrategy` </font>) - 这告诉我们如何将 $q(\\boldsymbol u)$ 在诱导点值上的分布转化为 $q(\\boldsymbol f)$ 在输入 $\\mathbf X$ 的潜在函数值上的分布.\n",
    "\n",
    "这里我们使用 `VariationalStrategy` with `learn_inducing_points=True`，以及 `CholeskyVariationalDistribution`。这些是最直接和常见的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31233335ea11079",
   "metadata": {},
   "source": [
    "#### The GP model\n",
    "\n",
    "`ApproximateGP` 模型是gpytorch的最简单的近似推断模型，他用 `VariationalDistribution` 指定的分布来近似真实后验，通常是某种形式的多变量高斯分布。该模型定义了所需的所有变分参数，并将这些信息都隐藏起来。\n",
    "\n",
    "`ApproximateGP` 模型组件：\n",
    "- __init__: 它构造一个平均模块 (mean module)，一个内核模块 (kernel module)，一个变分分布对象 (variational distribution) 和一个变分策略对象 (variational strategy)。该方法还应该负责构造可能需要的任何其他模块。\n",
    "- forward: 输入 $n \\times d$ 的数据 $\\mathbf X$ 并返回一个 MultivariatNormal，其中包含在x处的 先验均值 和 协方差。即返回向量 $\\mu(x)$ 和 $n \\times n$ 的矩阵 $\\mathbf K_{XX}$ 表示GP的先验均值和协方差矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a417e7984034cd6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T11:28:01.408116Z",
     "start_time": "2024-04-07T11:28:01.387954Z"
    }
   },
   "source": [
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution,\n",
    "                                                   learn_inducing_locations=True)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "inducing_points = train_x[:500, :]  # 训练集的前500作为诱导点输入\n",
    "model = GPModel(inducing_points=inducing_points)\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()  # 高斯似然\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.cuda()\n",
    "#     likelihood = likelihood.cuda()\n",
    "model = model.to(device)\n",
    "likelihood = likelihood.to(device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "593cfca62b32179b",
   "metadata": {},
   "source": [
    "#### Training the Model\n",
    "\n",
    "与使用精确的GP边际对数似然不同，执行变分推理允许我们使用随机优化技术。对于这个例子，我们将进行一个epoch的训练。考虑到神经网络相对于数据集的大小较小，这应该足以达到与DKL论文中观察到的相当的准确性。\n",
    "\n",
    "优化循环与我们在更简单的教程中看到的循环不同，因为它涉及到对许多训练迭代(epoch)和小批量数据的循环。然而，基本过程是相同的:对于每个小批量，我们向前通过模型，计算损失(VariationalELBO或ELBO)，向后调用，并执行优化步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c61ffeb82f218685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T11:28:14.468821Z",
     "start_time": "2024-04-07T11:28:02.626484Z"
    }
   },
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 1 if smoke_test else 4\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.01)\n",
    "\n",
    "# Our loss object. We're using the VariationalELBO (marginal log likelihood --> ELBO)\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "\n",
    "epochs_iter = tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "for i in epochs_iter:\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    minibatch_iter = tqdm(train_loader, desc=\"Minibatch\", leave=False)\n",
    "    for x_batch, y_batch in minibatch_iter:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = -mll(output, y_batch)\n",
    "        minibatch_iter.set_postfix(loss=loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "520ff1c4506bc0e2",
   "metadata": {},
   "source": [
    "#### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c298c022c4addb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T11:28:18.201699Z",
     "start_time": "2024-04-07T11:28:18.064409Z"
    }
   },
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "means = torch.tensor([0.])\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        preds = model(x_batch)\n",
    "        means = torch.cat([means, preds.mean.cpu()])\n",
    "means = means[1:]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "753b76dfeb0d05f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T11:28:26.841337Z",
     "start_time": "2024-04-07T11:28:26.822566Z"
    }
   },
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(means - test_y.cpu()))))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a761b374894b2651",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7887c635029990d3",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "38593eb913351e64",
   "metadata": {},
   "source": [
    "## Modifying the Variational Strategy/Variational Distribution\n",
    "\n",
    "近似GP的预测分布：\n",
    "$$\\begin{aligned}\n",
    "    p(\\boldsymbol f(\\boldsymbol x_*)) = \\int_{\\boldsymbol u} p(\\boldsymbol f(\\boldsymbol x_*) | \\boldsymbol u)\\, q(\\boldsymbol u)\\, \\mathrm{d}\\boldsymbol u,\\quad q(\\boldsymbol u) = \\mathcal N(\\boldsymbol m, \\mathbf S)\n",
    "\\end{aligned}$$\n",
    "$\\boldsymbol u = \\boldsymbol f_Z$ 代表 $m$ 个诱导点处的函数值。$\\boldsymbol m \\in \\mathbb R^m, \\mathbf S \\in \\mathbb R^{m \\times m}$ 为可学习参数。 \n",
    "\n",
    "如果 $m$ (诱导点的数量)相当大，可学习参数的数量在 $\\mathbf S$ 会很笨重。此外，一个巨大的 $m$ 可能会使一些计算变得很慢。这里我们展示了几种使用不同的变分分布和变分策略来实现这一目标的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3112b63c6316b1b2",
   "metadata": {},
   "source": [
    "### Experimental Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f0d21765a562717",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T11:43:44.646508Z",
     "start_time": "2024-04-07T11:43:44.553634Z"
    }
   },
   "source": [
    "import tqdm\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "\n",
    "if not smoke_test and not os.path.isfile('./datasets/elevators.mat'):\n",
    "    print('Downloading \\'elevators\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://drive.google.com/uc?export=download&id=1jhWL3YUHvXIaftia4qeAyDwVxo6j1alk',\n",
    "                               './datasets/elevators.mat')  # 这里 mat 文件需要手动从链接中下载\n",
    "\n",
    "if smoke_test:  # this is for running the notebook in our testing framework\n",
    "    X, y = torch.randn(1000, 3), torch.randn(1000)\n",
    "else:\n",
    "    data = torch.Tensor(loadmat('./datasets/elevators.mat')['data'])\n",
    "    X = data[:, :-1]\n",
    "    X = X - X.min(0)[0]\n",
    "    X = 2 * (X / X.max(0)[0]) - 1\n",
    "    y = data[:, -1]\n",
    "\n",
    "train_n = int(floor(0.8 * len(X)))\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "# if torch.cuda.is_available():\n",
    "# train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()\n",
    "train_x, train_y, test_x, test_y = train_x.to(device), train_y.to(device), test_x.to(device), test_y.to(device)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)  # original: batch_size=1024\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8ba873f1eb2fa3e7",
   "metadata": {},
   "source": [
    "### Some Quick Training/Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c3000802cb06ff4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T11:44:04.379023Z",
     "start_time": "2024-04-07T11:44:04.362365Z"
    }
   },
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "num_epochs = 1 if smoke_test else 10\n",
    "\n",
    "\n",
    "# Our testing script takes in a GPyTorch MLL (objective function) class\n",
    "# and then trains/tests an approximate GP with it on the supplied dataset\n",
    "\n",
    "def train_and_test_approximate_gp(model_cls):\n",
    "    inducing_points = torch.randn(128, train_x.size(-1), dtype=train_x.dtype, device=train_x.device)\n",
    "    model = model_cls(inducing_points)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.numel())\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()) + list(likelihood.parameters()), lr=0.1)\n",
    "\n",
    "    # if torch.cuda.is_available():\n",
    "    #     model = model.cuda()\n",
    "    #     likelihood = likelihood.cuda()\n",
    "    model = model.to(device)\n",
    "    likelihood = likelihood.to(device)\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    epochs_iter = tqdm(range(num_epochs), desc=f\"Training {model_cls.__name__}\")\n",
    "    for i in epochs_iter:\n",
    "        # Within each iteration, we will go over each minibatch of data\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            epochs_iter.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            preds = model(x_batch)\n",
    "            means = torch.cat([means, preds.mean.cpu()])\n",
    "    means = means[1:]\n",
    "    error = torch.mean(torch.abs(means - test_y.cpu()))\n",
    "    print(f\"Test {model_cls.__name__} MAE: {error.item()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "87cc7223c5291e9c",
   "metadata": {},
   "source": [
    "#### The Standard Approach\n",
    "\n",
    "默认情况下，我们将使用带有 ChooleskyVariationalDistribution 的默认 VariationalStrategy 类。`CholeskyVariationalDistribution` 类允许在任何正半定矩阵 $\\mathbf S$ 上。这是近似GP的最普遍/最具表现力的选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "974cf890b7a44fd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T11:44:06.617389Z",
     "start_time": "2024-04-07T11:44:06.610644Z"
    }
   },
   "source": [
    "class StandardApproximateGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(-2))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "897f6ac0fcbf5dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T11:44:12.537032Z",
     "start_time": "2024-04-07T11:44:07.176047Z"
    }
   },
   "source": [
    "train_and_test_approximate_gp(StandardApproximateGP)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "140569e68421ec30",
   "metadata": {},
   "source": [
    "### Reducing Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae667c79d3bd541",
   "metadata": {},
   "source": [
    "#### MeanFieldVariationalDistribution: a diagonal $\\mathbf S$ matrix\n",
    "\n",
    "减少参数数量的一种方法是对其进行限制：令 $\\mathbf S$ 是对角矩阵。这不是很有表现力，但是参数的数量现在是 $m$ 的线性的而不是二次。\n",
    "\n",
    "将 `CholeskyVariationalDistribution` (full $\\mathbf S$ 矩阵) 改成 `MeanFieldVariationalDistribution` (diagonal $\\mathbf S$ 矩阵)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ad57490be40b1d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T12:07:28.412846Z",
     "start_time": "2024-04-07T12:07:28.395752Z"
    }
   },
   "source": [
    "class MeanFieldApproximateGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.MeanFieldVariationalDistribution(inducing_points.size(-2))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e8462969754f2de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T12:07:39.881381Z",
     "start_time": "2024-04-07T12:07:35.738563Z"
    }
   },
   "source": [
    "train_and_test_approximate_gp(MeanFieldApproximateGP)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "efc396f8a6672e99",
   "metadata": {},
   "source": [
    "#### DeltaVariationalDistribution: no $\\mathbf S$ matrix\n",
    "\n",
    "一种更加极端的减少参数的方法是完全去掉 $\\mathbf S$ 矩阵。这对应于学习一个 delta distribution ($\\boldsymbol u = \\boldsymbol m$) 而不是 $\\boldsymbol u$ 的多变量的高斯分布 (multivariate normal distribution)。换句话说，这对应于 **执行MAP估计而不是变分推理**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a47889908d4ca576",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T12:25:19.302627Z",
     "start_time": "2024-04-07T12:25:19.260922Z"
    }
   },
   "source": [
    "class MAPApproximateGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.DeltaVariationalDistribution(inducing_points.size(-2))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ce181707fd415c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T12:25:28.017237Z",
     "start_time": "2024-04-07T12:25:24.244559Z"
    }
   },
   "source": [
    "train_and_test_approximate_gp(MAPApproximateGP)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "173cba3f4d44516",
   "metadata": {},
   "source": [
    "#### Reducing computation (through decoupled inducing points)\n",
    "\n",
    "降低计算复杂度的一种方法是分别对均值和协方差进行诱导点计算。\n",
    "\n",
    "在GPyTorch中，我们以模块化的方式实现此方法。正交解耦变分策略 OrthogonallyDecoupledVariationalStrategy 定义了均值诱导点的变分策略。它包含了一个定义协方差诱导点的现有变分策略/分布:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cd8c4a0acc505dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T12:27:27.690632Z",
     "start_time": "2024-04-07T12:27:27.683775Z"
    }
   },
   "source": [
    "def make_orthogonal_vs(model, train_x):\n",
    "    mean_inducing_points = torch.randn(1000, train_x.size(-1), dtype=train_x.dtype, device=train_x.device)\n",
    "    covar_inducing_points = torch.randn(100, train_x.size(-1), dtype=train_x.dtype, device=train_x.device)\n",
    "\n",
    "    covar_variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "        model, covar_inducing_points,\n",
    "        gpytorch.variational.CholeskyVariationalDistribution(covar_inducing_points.size(-2)),\n",
    "        learn_inducing_locations=True\n",
    "    )\n",
    "\n",
    "    variational_strategy = gpytorch.variational.OrthogonallyDecoupledVariationalStrategy(\n",
    "        covar_variational_strategy, mean_inducing_points,\n",
    "        gpytorch.variational.DeltaVariationalDistribution(mean_inducing_points.size(-2)),\n",
    "    )\n",
    "    return variational_strategy"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac486cc709fb3c40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T12:29:34.064683Z",
     "start_time": "2024-04-07T12:29:21.780556Z"
    }
   },
   "source": [
    "class OrthDecoupledApproximateGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.DeltaVariationalDistribution(inducing_points.size(-2))\n",
    "        variational_strategy = make_orthogonal_vs(self, train_x)\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "train_and_test_approximate_gp(OrthDecoupledApproximateGP)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc83f2547f4c475",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "52b94648dd519f4a",
   "metadata": {},
   "source": [
    "## Using Natural Gradient Descent with Variational Models\n",
    "\n",
    "### Overview\n",
    "\n",
    "优化变分GPyTorch模型时使用自然梯度下降。这将与SVGP回归笔记本相同，只是我们将使用不同的优化器。\n",
    "\n",
    "### What is natural gradient descent (NGD)?\n",
    "\n",
    "使用SGD或Adam并不是优化变分高斯分布参数的最佳方法。本质上，SGD采取的步骤假设参数的损失几何是欧几里得的。对于许多分布的参数来说，这是一个糟糕的假设，尤其是高斯分布。请参阅[Agustinus Kristiadi的博客文章](https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/)了解更多细节。\n",
    "\n",
    "相反，采用**更适合变分分布参数几何形状**的梯度步骤更有意义。具体来说，如果 $boldsymbol m$ 和 $\\mathbf S$ 是变分高斯后验近似的均值和协方差，那么如果我们采取以下步骤，我们将会实现更快收敛：\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        \\boldsymbol m \\\\\n",
    "        \\mathbf S \\\\\n",
    "    \\end{bmatrix} \\leftarrow\n",
    "    \\begin{bmatrix}\n",
    "        \\boldsymbol m \\\\\n",
    "        \\mathbf S \\\\\n",
    "    \\end{bmatrix} - \\alpha \\mathbf F^{-1} \\nabla \n",
    "    \\begin{bmatrix}\n",
    "        \\boldsymbol m \\\\\n",
    "        \\mathbf S\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "其中 $\\alpha$ 是步长，$\\mathbf F$ 是这个分布对应的 **Fisher information matrix** (费雪信息矩阵)。这被称为自然梯度下降 NGD。\n",
    "\n",
    "事实证明，对于高斯分布(更广泛地说，对于指数族中的所有分布)，存在有效的NGD更新方程。有关更多信息，请参阅以下文件: [Salimbeni, Hugh, Stefanos Eleftheriadis, and James Hensman. “Natural gradients in practice: Non-conjugate variational inference in gaussian process models.” AISTATS (2018). - Hensman, James, Magnus Rattray, and Neil D. Lawrence. “Fast variational inference in the conjugate exponential family.” NeurIPS (2012).](https://arxiv.org/abs/1803.09151)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f8b76e4cc7e692",
   "metadata": {},
   "source": [
    "### Jointly optimizing variational parameters/hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22026fbf59cf6458",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T13:40:06.179479Z",
     "start_time": "2024-04-07T13:40:03.299278Z"
    }
   },
   "source": [
    "# import tqdm\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "\n",
    "if not smoke_test and not os.path.isfile('./datasets/elevators.mat'):\n",
    "    print('Downloading \\'elevators\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://drive.google.com/uc?export=download&id=1jhWL3YUHvXIaftia4qeAyDwVxo6j1alk',\n",
    "                               './datasets/elevators.mat')  # 这里 mat 文件需要手动从链接中下载\n",
    "\n",
    "if smoke_test:  # this is for running the notebook in our testing framework\n",
    "    X, y = torch.randn(1000, 3), torch.randn(1000)\n",
    "else:\n",
    "    data = torch.Tensor(loadmat('./datasets/elevators.mat')['data'])\n",
    "    X = data[:, :-1]\n",
    "    X = X - X.min(0)[0]\n",
    "    X = 2 * (X / X.max(0)[0]) - 1\n",
    "    y = data[:, -1]\n",
    "\n",
    "train_n = int(floor(0.8 * len(X)))\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "# if torch.cuda.is_available():\n",
    "# train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()\n",
    "train_x, train_y, test_x, test_y = train_x.to(device), train_y.to(device), test_x.to(device), test_y.to(device)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)  # original: batch_size=1024\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "add558915dea96",
   "metadata": {},
   "source": [
    "### SVGP models for NGD\n",
    "\n",
    "NGD-SVGP模型与来自SVGP回归笔记本的标准SVGP模型之间有三个关键区别。\n",
    "\n",
    "#### Difference 1: NaturalVariationalDistribution\n",
    "\n",
    "不是使用gpytorch.variation.CholeskyVarationalDistribution(或其他变分分布对象)，你必须使用以下两个对象之一:\n",
    "- `gpytorch.variational.NaturalVariationalDistribution` 通常更快的优化收敛，但对非共轭似然不太稳定\n",
    "- `gpytorch.variational.TrilNaturalVariationalDistribution` 通常较慢的优化收敛，但对非共轭似然更稳定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e38bb07a11432938",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T13:40:11.297184Z",
     "start_time": "2024-04-07T13:40:11.285662Z"
    }
   },
   "source": [
    "class GPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.NaturalVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "inducing_points = train_x[:500, :]\n",
    "model = GPModel(inducing_points=inducing_points)\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.cuda()\n",
    "#     likelihood = likelihood.cuda()\n",
    "model = model.to(device)\n",
    "likelihood = likelihood.to(device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7e1976ffd7f744b0",
   "metadata": {},
   "source": [
    "#### Difference 2: Two optimizers - one for the variational parameters; one for the hyperparameters\n",
    "\n",
    "NGD步骤只更新变分参数。因此，我们需要两个独立的优化器:一个用于变分参数(使用NGD)，另一个用于其他超参数(使用Adam或任何您想要的)。\n",
    "\n",
    "关于NGD变分优化器需要注意的一些事项:\n",
    "- 你必须使用gpytorch.optim。作为变分NGD优化器!自适应梯度算法会打乱自然梯度步骤。(任何随机优化器都适用于超参数。)\n",
    "- 对变分优化器使用较大的学习率。一般来说，0.1是一个很好的学习率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "543dd5f39faa7d9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T13:40:12.482360Z",
     "start_time": "2024-04-07T13:40:12.477843Z"
    }
   },
   "source": [
    "variational_ngd_optimizer = gpytorch.optim.NGD(model.variational_parameters(), num_data=train_y.size(0), lr=0.1)\n",
    "\n",
    "hyperparameter_optimizer = torch.optim.Adam([\n",
    "    {'params': model.hyperparameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.01)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "819a3c51fbda9c89",
   "metadata": {},
   "source": [
    "#### Difference 3: The updated training loop\n",
    "\n",
    "在训练循环中，我们必须更新两个优化器:\n",
    "- `variational_ngd_optimizer`\n",
    "- `hyperparameter_optimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca06d2e7b3dec0fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T13:40:34.887049Z",
     "start_time": "2024-04-07T13:40:24.221575Z"
    }
   },
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "\n",
    "num_epochs = 1 if smoke_test else 4\n",
    "epochs_iter = tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "for i in epochs_iter:\n",
    "    minibatch_iter = tqdm(train_loader, desc=\"Minibatch\", leave=False)\n",
    "    # minibatch_iter = tqdm(train_loader, desc=\"Minibatch\", )\n",
    "\n",
    "    for x_batch, y_batch in minibatch_iter:\n",
    "        ### Perform NGD step to optimize variational parameters\n",
    "        variational_ngd_optimizer.zero_grad()\n",
    "        hyperparameter_optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = -mll(output, y_batch)\n",
    "        minibatch_iter.set_postfix(loss=loss.item())\n",
    "        loss.backward()\n",
    "        variational_ngd_optimizer.step()  # 更新 变分优化器\n",
    "        hyperparameter_optimizer.step()  # 更新 超参优化器"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e2ebb581db0d145b",
   "metadata": {},
   "source": [
    "你也可以修改优化循环，让它在NGD步骤和超参数更新之间交替进行:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e79d43851b074d7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T13:40:45.471728Z",
     "start_time": "2024-04-07T13:40:41.411772Z"
    }
   },
   "source": [
    "for x_batch, y_batch in minibatch_iter:\n",
    "    ### Perform NGD step to optimize variational parameters\n",
    "    variational_ngd_optimizer.zero_grad()\n",
    "    output = model(x_batch)\n",
    "    loss = -mll(output, y_batch)\n",
    "    minibatch_iter.set_postfix(loss=loss.item())\n",
    "    loss.backward()\n",
    "    variational_ngd_optimizer.step()\n",
    "\n",
    "    ### Perform Adam step to optimize hyperparameters\n",
    "    # 交替优化\n",
    "    hyperparameter_optimizer.zero_grad()\n",
    "    output = model(x_batch)\n",
    "    loss = -mll(output, y_batch)\n",
    "    loss.backward()\n",
    "    hyperparameter_optimizer.step()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "07f4b20b",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "这个模型应该比标准的SVGP模型收敛得更快/更好——而且它通常可以获得更好的性能(特别是当使用更多的诱导点时)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52a4c88a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T13:41:30.112843Z",
     "start_time": "2024-04-07T13:41:29.985711Z"
    }
   },
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "means = torch.tensor([0.])\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        preds = model(x_batch)\n",
    "        means = torch.cat([means, preds.mean.cpu()])\n",
    "means = means[1:]\n",
    "print('Test MAE: {}'.format(torch.mean(torch.abs(means - test_y.cpu()))))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c51cd21a",
   "metadata": {},
   "source": [
    "## Stochastic Variational GP Regression with Contour Integral Quadrature\n",
    "\n",
    "具有等值积分正交的随机变分GP回归\n",
    "\n",
    "### Overview\n",
    "\n",
    "如何使用Pleiss等人，2020中描述的轮廓积分正交(CIQ)和msMINRES进行随机变分GP回归。在下列情况下，可以用轮廓积分正交代替标准SVGP:\n",
    "- 诱导点很多(如M > 5000)\n",
    "- 诱导点具有特殊的结构(如位于网格上)。\n",
    "\n",
    "我们将概述如何使用[CIQ-SVGP随机变分回归](https://arxiv.org/pdf/1411.2005.pdf)在3droad UCI数据集上使用minibatch快速训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aff825f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T13:53:40.358354Z",
     "start_time": "2024-04-07T13:53:40.141658Z"
    }
   },
   "source": [
    "# import tqdm\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "\n",
    "if not smoke_test and not os.path.isfile('./datasets/3droad.mat'):\n",
    "    print('Downloading \\'3droad\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://www.dropbox.com/s/f6ow1i59oqx05pl/3droad.mat?dl=1', './datasets/3droad.mat')  # 这里 mat 文件需要手动从链接中下载\n",
    "\n",
    "if smoke_test:  # this is for running the notebook in our testing framework\n",
    "    X, y = torch.randn(10, 2), torch.randn(10)\n",
    "else:\n",
    "    data = torch.Tensor(loadmat('./datasets/3droad.mat')['data'])\n",
    "    X = data[:, :-2]\n",
    "    X = X - X.min(0)[0]\n",
    "    X = 2 * (X / X.max(0)[0]) - 1\n",
    "    y = data[:, -1]\n",
    "    y.sub_(y.mean(0)).div_(y.std(0))\n",
    "    \n",
    "    # Let's subsample the data\n",
    "    indices = torch.randperm(X.size(0))[:10000]\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "train_n = int(floor(0.8 * len(X)))\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "# if torch.cuda.is_available():\n",
    "# train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()\n",
    "train_x, train_y, test_x, test_y = train_x.to(device), train_y.to(device), test_x.to(device), test_y.to(device)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "18e635b5",
   "metadata": {},
   "source": [
    "### DataLoaders with CIQ-SVGP\n",
    "\n",
    "CIQ仅在小批量大小远小于诱导点数量时提供计算加速。我们发现256的小批量通常效果很好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0445904",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T13:53:41.636965Z",
     "start_time": "2024-04-07T13:53:41.624360Z"
    }
   },
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "# Smaller batch sizes are better for CIQ\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6429a13d",
   "metadata": {},
   "source": [
    "### Number of inducing points\n",
    "\n",
    "当有很多诱导点时，CIQ提供了加速计算。这里，我们选择2000个诱导点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80096c64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T13:53:44.892941Z",
     "start_time": "2024-04-07T13:53:42.659560Z"
    }
   },
   "source": [
    "inducing_points = train_x[torch.randperm(train_x.size(0))[:2000]]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bd7262f1",
   "metadata": {},
   "source": [
    "### CIQ - SVGP models\n",
    "\n",
    "要使用轮廓积分正交，只需将 `VariationalStrategy` 替换为 `CiqVariationalStrategy`。\n",
    "\n",
    "在这个例子中，我们使用的是 `NaturalVariationalStrategy`，因为CIQ在自然梯度下降的情况下效果最好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a987a3a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T13:53:45.913863Z",
     "start_time": "2024-04-07T13:53:45.874475Z"
    }
   },
   "source": [
    "class GPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.NaturalVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.CiqVariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=2.5, ard_num_dims=2)\n",
    "        )\n",
    "        self.covar_module.base_kernel.initialize(lengthscale=0.01)  # Specific to the 3droad dataset\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "model = GPModel(inducing_points=inducing_points)\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.cuda()\n",
    "#     likelihood = likelihood.cuda()\n",
    "model = model.to(device)\n",
    "likelihood = likelihood.to(device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2df6051",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T13:53:46.897894Z",
     "start_time": "2024-04-07T13:53:46.887680Z"
    }
   },
   "source": [
    "variational_ngd_optimizer = gpytorch.optim.NGD(model.variational_parameters(), num_data=train_y.size(0), lr=0.01)\n",
    "\n",
    "hyperparameter_optimizer = torch.optim.Adam([\n",
    "    {'params': model.hyperparameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.01)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95f5f3dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T13:58:11.491318Z",
     "start_time": "2024-04-07T13:53:48.471657Z"
    },
    "scrolled": false
   },
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "\n",
    "num_epochs = 1 if smoke_test else 4\n",
    "epochs_iter = tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "for i in epochs_iter:\n",
    "    minibatch_iter = tqdm(train_loader, desc=\"Minibatch\", leave=False)\n",
    "\n",
    "    for x_batch, y_batch in minibatch_iter:\n",
    "        variational_ngd_optimizer.zero_grad()\n",
    "        hyperparameter_optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = -mll(output, y_batch)\n",
    "        minibatch_iter.set_postfix(loss=loss.item())\n",
    "        loss.backward()\n",
    "        variational_ngd_optimizer.step()\n",
    "        hyperparameter_optimizer.step()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "baa6986b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:07:05.269921Z",
     "start_time": "2024-04-07T14:07:02.774216Z"
    }
   },
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "means = torch.tensor([0.])\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        preds = model(x_batch)\n",
    "        means = torch.cat([means, preds.mean.cpu()])\n",
    "means = means[1:]\n",
    "print('Test MAE: {}'.format(torch.mean(torch.abs(means - test_y.cpu()))))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c75976e",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "271.892px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
