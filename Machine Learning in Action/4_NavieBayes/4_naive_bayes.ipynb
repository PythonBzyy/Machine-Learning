{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24178303",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T09:34:20.822359Z",
     "start_time": "2023-09-28T09:34:20.817187Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d646ac0",
   "metadata": {},
   "source": [
    "## 准备：从文本制作词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34d1520",
   "metadata": {},
   "source": [
    "以词向量或标记向量的形式查看文本，即将句子转换为向量。\n",
    "\n",
    "将每个单独的文档转换为词汇表中的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7bc34a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T09:33:28.274028Z",
     "start_time": "2023-09-28T09:33:28.265679Z"
    }
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList = [\n",
    "        ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "        ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "        ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "        ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "        ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "        ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']\n",
    "    ]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]  #1 is abusive, 0 not\n",
    "    return postingList, classVec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0dbfb2",
   "metadata": {},
   "source": [
    "- 第一个函数创建一些示例数据进行实验。从 loadDatSet() 返回的第一个变量是来自达尔马提亚（斑点狗）爱好者留言板的一组标记化文档。文本已被分解为一组标记。标点符号也已从该文本中删除。\n",
    "- loadDatSet() 的第二个变量返回一组类标签。这里有两个类别，辱骂类和非辱骂类。\n",
    "- postingList - 实验样本切分的词条\n",
    "- classVec - 类别标签向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ee2e8b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T09:33:30.124071Z",
     "start_time": "2023-09-28T09:33:30.115065Z"
    }
   },
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  #create empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)  #union of the two sets\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd454a0",
   "metadata": {},
   "source": [
    "- 函数 createVocabList() 将创建所有文档中所有唯一单词的列表。首先，创建一个空集。接下来，将每个文档中的新集合附加到该集合中。\n",
    "- dataSet - 整理的样本数据集\n",
    "- vocabSet - 返回不重复的词条列表，也就是词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d2e52dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T09:33:31.637910Z",
     "start_time": "2023-09-28T09:33:31.627154Z"
    }
   },
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a76bc9",
   "metadata": {},
   "source": [
    "- vocabList - createVocabList返回的列表\n",
    "- inputSet - 切分的词条列表\n",
    "- returnVec - 文档向量,词集模型\n",
    "- 最后，在获得词汇列表后，使用函数 setOfWords2Vec()，该函数获取词汇列表和文档，并输出由 1 和 0 组成的向量来表示词汇表中的单词是否存在于给定文档中。然后创建一个与词汇列表长度相同的向量并用 0 填充它。 \n",
    "- 接下来浏览文档中的单词，如果该单词在词汇列表中，则在输出向量中将其值设置为 1。如果一切顺利，不需要测试某个单词是否在 vocabList 中，但稍后可能会使用它。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e367420",
   "metadata": {},
   "source": [
    "**函数实际效果**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43c296e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T09:33:32.569317Z",
     "start_time": "2023-09-28T09:33:32.553320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'has', 'flea', 'problems', 'help', 'please']\n",
      "['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid']\n",
      "['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him']\n",
      "['stop', 'posting', 'stupid', 'worthless', 'garbage']\n",
      "['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him']\n",
      "['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']\n",
      "[0, 1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "postingList, classVec = loadDataSet()\n",
    "for each in postingList:\n",
    "    print(each)\n",
    "print(classVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47293c3",
   "metadata": {},
   "source": [
    "- postingList是存放词条列表中，classVec是存放每个词条的所属类别，1代表侮辱类 ，0代表非侮辱类。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3503f3a9",
   "metadata": {},
   "source": [
    "## 训练：根据词向量计算概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b78bf34",
   "metadata": {},
   "source": [
    "得到词向量后，通过词向量训练朴素贝叶斯分类器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bb4d90e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T09:33:34.039165Z",
     "start_time": "2023-09-28T09:33:34.028248Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)  #计算训练的文档数目\n",
    "    numWords = len(trainMatrix[0])  #计算每篇文档的词条数\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)  #文档属于侮辱类的概率\n",
    "    p0Num = np.zeros(numWords)\n",
    "    p1Num = np.zeros(numWords)  #创建numpy.zeros数组,词条出现数初始化为0\n",
    "    p0Denom = 0.0\n",
    "    p1Denom = 0.0  #分母初始化为0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:  #统计属于侮辱类的条件概率所需的数据，即P(w0|1),P(w1|1),P(w2|1)···\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:  #统计属于非侮辱类的条件概率所需的数据，即P(w0|0),P(w1|0),P(w2|0)···\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = p1Num / p1Denom\n",
    "    p0Vect = p0Num / p0Denom\n",
    "    return p0Vect, p1Vect, pAbusive  #返回属于侮辱类的条件概率数组，属于非侮辱类的条件概率数组，文档属于侮辱类的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b9221b",
   "metadata": {},
   "source": [
    "- trainMatrix - 训练文档矩阵，即setOfWords2Vec返回的returnVec构成的矩阵\n",
    "- trainCategory - 训练类别标签向量，即loadDataSet返回的classVec\n",
    "- p0Vect - 侮辱类的条件概率数组\n",
    "- p1Vect - 非侮辱类的条件概率数组\n",
    "- pAbusive - 文档属于侮辱类的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64db843b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T09:33:35.497383Z",
     "start_time": "2023-09-28T09:33:35.477519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myVocabList:\n",
      " ['take', 'to', 'ate', 'please', 'worthless', 'stop', 'flea', 'maybe', 'cute', 'has', 'dog', 'stupid', 'steak', 'quit', 'buying', 'not', 'so', 'posting', 'how', 'food', 'mr', 'garbage', 'park', 'licks', 'I', 'dalmation', 'is', 'him', 'my', 'problems', 'love', 'help']\n",
      "p0V:\n",
      " [0.         0.04166667 0.04166667 0.04166667 0.         0.04166667\n",
      " 0.04166667 0.         0.04166667 0.04166667 0.04166667 0.\n",
      " 0.04166667 0.         0.         0.         0.04166667 0.\n",
      " 0.04166667 0.         0.04166667 0.         0.         0.04166667\n",
      " 0.04166667 0.04166667 0.04166667 0.08333333 0.125      0.04166667\n",
      " 0.04166667 0.04166667]\n",
      "p1V:\n",
      " [0.05263158 0.05263158 0.         0.         0.10526316 0.05263158\n",
      " 0.         0.05263158 0.         0.         0.10526316 0.15789474\n",
      " 0.         0.05263158 0.05263158 0.05263158 0.         0.05263158\n",
      " 0.         0.05263158 0.         0.05263158 0.05263158 0.\n",
      " 0.         0.         0.         0.05263158 0.         0.\n",
      " 0.         0.        ]\n",
      "classVec:\n",
      " [0, 1, 0, 1, 0, 1]\n",
      "pAb:\n",
      " 0.5\n"
     ]
    }
   ],
   "source": [
    "postingList, classVec = loadDataSet()\n",
    "myVocabList = createVocabList(postingList)\n",
    "print('myVocabList:\\n', myVocabList)\n",
    "trainMat = []\n",
    "for postinDoc in postingList:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "p0V, p1V, pAb = trainNB0(trainMat, classVec)\n",
    "print('p0V:\\n', p0V)\n",
    "print('p1V:\\n', p1V)\n",
    "print('classVec:\\n', classVec)\n",
    "print('pAb:\\n', pAb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2663e04d",
   "metadata": {},
   "source": [
    "- p0V存放的是每个单词属于类别0，也就是非侮辱类词汇的概率。比如p0V的倒数第6个概率，就是stupid这个单词属于非侮辱类的概率为0。同理，p1V的倒数第6个概率，就是stupid这个单词属于侮辱类的概率为0.15789474，也就是约等于15.79%的概率。\n",
    "- stupid的中文意思是蠢货，显而易见，这个单词属于侮辱类。\n",
    "- pAb是所有侮辱类的样本占所有样本的概率，从classVec中可以看出，一用有3个侮辱类，3个非侮辱类。所以侮辱类的概率是0.5。因此p0V存放的就是P(him|非侮辱类) = 0.0833、P(is|非侮辱类) = 0.0417，一直到P(dog|非侮辱类) = 0.0417，这些单词的条件概率。同理，p1V存放的就是各个单词属于侮辱类的条件概率。pAb就是先验概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe97a32c",
   "metadata": {},
   "source": [
    "## 测试：根据现实条件修改分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cc01fde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T09:33:37.417770Z",
     "start_time": "2023-09-28T09:33:37.403168Z"
    }
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = reduce(lambda x, y: x * y, vec2Classify * p1Vec) * pClass1  #对应元素相乘\n",
    "    p0 = reduce(lambda x, y: x * y, vec2Classify * p0Vec) * (1.0 - pClass1)\n",
    "    print('p0:', p0)\n",
    "    print('p1:', p1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d849a8eb",
   "metadata": {},
   "source": [
    "- vec2Classify - 待分类的词条数组\n",
    "- p0Vec - 非侮辱类的条件概率数组\n",
    "- p1Vec -侮辱类的条件概率数组\n",
    "- pClass1 - 文档属于侮辱类的概率\n",
    "- returns: 0 - 属于非侮辱类; 1 - 属于侮辱类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aeb975bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T09:33:38.348576Z",
     "start_time": "2023-09-28T09:33:38.339365Z"
    }
   },
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    listOPosts, listClasses = loadDataSet()  #创建实验样本\n",
    "    myVocabList = createVocabList(listOPosts)  #创建词汇表\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))  #将实验样本向量化\n",
    "    p0V, p1V, pAb = trainNB0(np.array(trainMat),\n",
    "                             np.array(listClasses))  #训练朴素贝叶斯分类器\n",
    "    testEntry = ['love', 'my', 'dalmation']  #测试样本1\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))  #测试样本向量化\n",
    "    if classifyNB(thisDoc, p0V, p1V, pAb):\n",
    "        print(testEntry, '属于侮辱类')  #执行分类并打印分类结果\n",
    "    else:\n",
    "        print(testEntry, '属于非侮辱类')  #执行分类并打印分类结果\n",
    "    testEntry = ['stupid', 'garbage']  #测试样本2\n",
    "\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))  #测试样本向量化\n",
    "    if classifyNB(thisDoc, p0V, p1V, pAb):\n",
    "        print(testEntry, '属于侮辱类')  #执行分类并打印分类结果\n",
    "    else:\n",
    "        print(testEntry, '属于非侮辱类')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9058844a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T09:33:38.862422Z",
     "start_time": "2023-09-28T09:33:38.852879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0: 0.0\n",
      "p1: 0.0\n",
      "['love', 'my', 'dalmation'] 属于非侮辱类\n",
      "p0: 0.0\n",
      "p1: 0.0\n",
      "['stupid', 'garbage'] 属于非侮辱类\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357943a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T11:07:30.437307Z",
     "start_time": "2023-09-27T11:07:30.431306Z"
    }
   },
   "source": [
    "- 创建了两个测试样本：['love', 'my', 'dalmation']和['stupid', 'garbage']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5a64a",
   "metadata": {},
   "source": [
    "**Problems：**\n",
    "- 发现无法正确分类，原因：利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算p(w0|1)p(w1|1)p(w2|1)。如果其中有一个概率值为0，那么最后的成绩也为0。\n",
    "- 除此之外，另外一个遇到的问题就是**下溢出**，这是由于太多很小的数相乘造成的。为了解决这个问题，对乘积结果取**自然对数**。通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed5309e",
   "metadata": {},
   "source": [
    "修改test函数和classifyNB函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d5cab18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T09:33:40.534426Z",
     "start_time": "2023-09-28T09:33:40.515562Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)  #计算训练的文档数目\n",
    "    numWords = len(trainMatrix[0])  #计算每篇文档的词条数\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)  #文档属于侮辱类的概率\n",
    "    p0Num = np.ones(numWords)\n",
    "    p1Num = np.ones(numWords)  #创建numpy.ones数组,词条出现数初始化为1，拉普拉斯平滑\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0  #分母初始化为2,拉普拉斯平滑\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:  #统计属于侮辱类的条件概率所需的数据，即P(w0|1),P(w1|1),P(w2|1)···\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:  #统计属于非侮辱类的条件概率所需的数据，即P(w0|0),P(w1|0),P(w2|0)···\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = np.log(p1Num / p1Denom)  #取对数，防止下溢出\n",
    "    p0Vect = np.log(p0Num / p0Denom)\n",
    "    return p0Vect, p1Vect, pAbusive\n",
    "\n",
    "\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(\n",
    "        pClass1)  #对应元素相乘。logA * B = logA + logB\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da6a4c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T09:33:41.059793Z",
     "start_time": "2023-09-28T09:33:41.052606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] 属于非侮辱类\n",
      "['stupid', 'garbage'] 属于侮辱类\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b0bbe",
   "metadata": {},
   "source": [
    "**结果正确！！！**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34ce7e",
   "metadata": {},
   "source": [
    "## 过滤垃圾邮件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b46ebeaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T09:33:42.082387Z",
     "start_time": "2023-09-28T09:33:42.064992Z"
    }
   },
   "outputs": [],
   "source": [
    "# 接收一个 大字符串 并将其解析为 字符串列表\n",
    "def textParse(bigString):  #将字符串转换为字符列表\n",
    "    listOfTokens = re.split(r'\\W', bigString)  #将特殊符号作为切分标志进行字符串切分，即非字母、非数字\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]  #除了单个字母，例如大写的I，其它单词变成小写\n",
    "\n",
    "\n",
    "# 将切分的实验样本词条整理成不重复的词条列表，也就是词汇表\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  #创建一个空的不重复列表\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)  #取并集\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37db5c1f",
   "metadata": {},
   "source": [
    "- 原书中re.split(r'\\W*')匹配任何非字母数字字符的序列，包括没有字符的情况（即空字符串，0长度），因此无法成功运行，这里改为'\\W'即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eeb22bdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T09:33:43.022660Z",
     "start_time": "2023-09-28T09:33:43.001852Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['color', 'email', 'art', 'benoit', 'because', 'plus', 'gains', '90563', 'attaching', 'leaves', 'individual', 'life', 'how', '50mg', 'year', 'way', 'will', 'over', 'inform', 'been', 'moderately', 'pharmacy', 'help', 'sites', 'logged', 'development', 'save', 'mathematician', 'forum', 'received', 'item', 'now', 'yeah', 'ryan', '292', 'tent', 'pictures', 'ideas', 'volume', 'bargains', '5mg', 'saw', 'and', 'doing', 'sophisticated', 'new', 'low', 'with', 'tesla', 'from', 'that', 'roofer', 'watson', 'approved', 'shipping', 'jocelyn', 'them', 'ordercializviagra', 'told', 'competitive', '513', 'release', 'most', 'fans', 'night', 'wilmott', 'see', 'such', 'fedex', 'pick', 'place', 'window', 'hello', 'going', 'went', 'out', 'then', 'inside', 'take', 'windows', 'zolpidem', 'pricing', 'comment', '100m', 'let', 'free', '2007', 'gucci', 'concise', '100', 'www', 'buyviagra', 'cca', 'wholesale', 'reply', 'wilson', 'decision', 'cold', 'strategic', 'assistance', 'have', 'discreet', 'shipment', 'storedetailview_98', 'sky', '203', 'keep', '129', 'credit', 'freeviagra', 'haloney', 'all', 'may', 'pro', 'ambiem', 'address', 'finance', '100mg', 'modelling', 'lunch', 'capabilities', 'fine', 'doctor', 'huge', 'linkedin', 'also', 'faster', 'back', 'opportunity', 'discussions', 'professional', 'using', 'thread', '291', 'today', 'withoutprescription', 'longer', '2011', 'cats', 'management', 'girl', 'major', 'famous', 'analgesic', 'discount', 'prototype', 'zach', '66343', 'being', 'butt', 'extended', 'jquery', 'oris', 'retirement', 'town', 'access', 'interesting', 'but', 'looking', 'placed', 'had', 'explosive', 'harderecetions', 'cost', 'featured', 'ems', 'python', 'bags', 'support', 'reputable', 'group', 'possible', 'jqplot', 'answer', 'blue', 'specifically', 'delivery', 'more', 'you', 'pills', 'status', 'couple', 'time', '2010', 'use', 'made', '625', 'tour', 'stepp', '750', 'selected', 'changes', 'those', 'pill', 'both', 'welcome', 'two', 'foaming', 'troy', 'drugs', 'cheap', 'location', 'thought', 'com', 'arvind', 'bike', 'thailand', 'favorite', 'right', 'yesterday', 'writing', 'style', 'superb', 'game', 'cuda', 'you抮e', 'held', '174623', 'supplement', 'narcotic', 'earn', 'others', 'sure', 'assigning', 'money', 'per', 'exhibit', 'issues', 'shape', 'work', 'for', 'they', 'betterejacu1ation', 'inches', 'finder', 'adobe', 'focus', 'oem', 'inconvenience', 'owner', 'link', 'borders', 'femaleviagra', '14th', 'tiffany', 'number', 'might', 'china', 'automatic', 'need', 'past', 'groups', 'rude', 'quantitative', 'cannot', 'forward', 'runs', 'coast', 'book', 'office', 'sent', 'safe', 'stuff', 'online', 'the', 'latest', 'incredib1e', 'designed', 'commented', 'perhaps', '225', 'turd', 'grounds', 'dusty', 'rent', 'network', 'great', 'endorsed', 'phone', 'yay', 'who', 'talked', 'has', 'jewerly', 'copy', 'hope', 'guaranteeed', 'creative', 'uses', 'http', 'phentermin', 'store', 'jay', 'day', 'holiday', 'message', 'natural', 'files', 'working', '130', 'program', 'order', 'check', 'survive', 'important', 'any', 'based', 'encourage', 'think', 'don', 'ultimate', 'vicodin', 'mandarin', 'reliever', 'try', 'core', 'thickness', 'once', 'fast', 'hommies', 'ferguson', '562', 'automatically', 'hotels', 'hold', 'add', 'create', 'riding', 'know', 'just', 'express', 'should', 'transformed', 'fda', 'derivatives', 'approach', 'products', 'tokyo', 'fractal', 'full', 'this', '30mg', 'gpu', 'regards', 'monte', 'customized', 'winter', 'wrote', 'insights', 'mathematics', '300x', 'millions', 'knew', 'series', 'specifications', 'prices', 'visa', 'fermi', 'tickets', 'methylmorphine', 'february', 'listed', 'gain', 'chapter', '180', 'effective', 'thirumalai', 'brained', 'notification', 'sorry', 'bin', 'carlo', 'functionalities', 'courier', 'control', 'easily', 'docs', 'having', 'cat', 'name', 'october', 'level', 'brand', 'quality', 'magazine', 'vivek', 'drunk', 'glimpse', 'hotel', 'thank', 'invitation', 'dozen', 'high', 'train', 'prepared', 'supporting', 'worldwide', 'giants', '120', 'share', '396', 'thing', 'accepted', 'top', 'things', 'required', 'pages', 'watchesstore', 'dior', 'ups', 'generation', 'computing', 'chinese', 'experience', 'acrobat', 'serial', '430', 'well', 'biggerpenis', 'hydrocodone', 'thousand', 'died', 'advocate', 'viagranoprescription', 'vuitton', 'follow', 'want', 'lists', 'edit', 'certified', 'file', 'scenic', 'enough', 'cheers', 'page', 'rock', 'treat', 'service', 'improving', 'experts', 'horn', 'percocet', 'tabs', 'focusing', 'ma1eenhancement', 'food', 'father', 'mom', 'mailing', 'genuine', 'launch', 'your', 'got', '1924', 'while', 'here', 'cs5', 'heard', 'done', 'used', 'everything', 'located', 'don抰', '199', 'often', 'suggest', 'source', 'home', 'opioid', 'mandatory', 'intenseorgasns', 'differ', 'kerry', 'eugene', 'came', 'codeine', 'dhl', 'works', 'brands', 'below', 'reservation', 'inspired', 'school', 'safest', 'job', 'of_penisen1argement', 'whybrew', 'fbi', 'view', 'running', 'through', 'days', 'another', 'risk', 'pretty', 'either', 'louis', 'success', 'ones', 'length', 'instead', 'hours', 'softwares', '492', 'hermes', 'changing', 'much', 'naturalpenisenhancement', 'ready', 'there', 'jpgs', 'like', 'thanks', 'sliding', 'google', 'cartier', 'than', 'since', 'doggy', 'away', 'incoming', 'strategy', 'proven', 'starting', 'brandviagra', 'far', '195', 'get', 'storage', '322', 'definitely', 'germany', '119', 'speedpost', 'trusted', 'articles', 'arolexbvlgari', 'was', 'canadian', 'price', 'herbal', 'where', 'not', 'class', 'call', 'warranty', 'parallel', 'wasn', 'cards', 'microsoft', 'york', 'station', 'only', 'expo', 'can', 'mail', 'grow', 'photoshop', 'yourpenis', '200', 'permanantly', 'buy', 'would', '219', 'connection', 'pain', 'plane', '156', 'enjoy', 'update', '385', 'come', 'when', 'john', 'knocking', 'hangzhou', 'trip', '10mg', 'plugin', 'income', 'contact', 'close', 'nature', 'does', '0nline', 'rain', 'bettererections', 'find', 'includes', 'recieve', 'expertise', 'announcement', 'hamm', 'methods', 'members', 'learn', 'tool', 'private', 'enabled', '138', 'province', 'spaying', 'example', 'chance', 'code', 'too', 'could', '325', 'what', 'upload', 'requested', 'please', 'model', 'meet', 'care', '588', 'doors', 'bad', '15mg', 'must', 'accept', 'lined', 'mandelbrot', 'aged', 'gas', 'titles', 'design', 'computer', 'these', 'amex', 'site', 'signed', 'noprescription', 'information', 'web', 'note', 'programming', 'julius', 'are', 'good', 'items', 'questions', 'party', 'creation', 'significantly', 'about', 'financial', 'least', 'one', 'features', '366', 'net', 'amazing', 'peter', 'nvidia', 'generates', 'same', '25mg', 'fundamental', 'door', 'watches', 'needed', 'ofejacu1ate', 'via', 'jose', 'moneyback', 'pavilion', 'off', 'pls', 'scifinance', '86152', 'each', 'mba', '50092', 'said', 'jar', 'sounds', 'car', '570', 'museum', 'business', 'increase', 'website', 'guy', 'download', 'his', 'behind', 'severepain', 'wednesday', 'team', 'moderate', 'some', 'bathroom', 'wallets']\n"
     ]
    }
   ],
   "source": [
    "docList = []\n",
    "classList = []\n",
    "for i in range(1, 26):  #遍历25个txt文件\n",
    "    wordList = textParse(open('./datasets/email/spam/%d.txt' % i,\n",
    "                              'r').read())  #读取每个垃圾邮件，并字符串转换成字符串列表\n",
    "    docList.append(wordList)\n",
    "    classList.append(1)  #标记垃圾邮件，1表示垃圾文件\n",
    "    wordList = textParse(open('./datasets/email/ham/%d.txt' % i,\n",
    "                              'r').read())  #读取每个非垃圾邮件，并字符串转换成字符串列表\n",
    "    docList.append(wordList)\n",
    "    classList.append(0)  #标记非垃圾邮件，1表示垃圾文件\n",
    "vocabList = createVocabList(docList)  #创建词汇表，不重复\n",
    "print(vocabList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab91218",
   "metadata": {},
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f835e3",
   "metadata": {},
   "source": [
    "将数据集分为训练集和测试集，使用交叉验证的方式测试朴素贝叶斯分类器的准确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3e8b6ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T09:33:44.325432Z",
     "start_time": "2023-09-28T09:33:44.314419Z"
    }
   },
   "outputs": [],
   "source": [
    "# 测试朴素贝叶斯分类器\n",
    "def spamTest():\n",
    "    docList = []\n",
    "    classList = []\n",
    "    fullText = []\n",
    "    for i in range(1, 26):  #遍历25个txt文件\n",
    "        wordList = textParse(open('./datasets/email/spam/%d.txt' % i,\n",
    "                                  'r').read())  #读取每个垃圾邮件，并字符串转换成字符串列表\n",
    "        docList.append(wordList)\n",
    "        fullText.append(wordList)\n",
    "        classList.append(1)  #标记垃圾邮件，1表示垃圾文件\n",
    "        wordList = textParse(open('./datasets/email/ham/%d.txt' % i,\n",
    "                                  'r').read())  #读取每个非垃圾邮件，并字符串转换成字符串列表\n",
    "        docList.append(wordList)\n",
    "        fullText.append(wordList)\n",
    "        classList.append(0)  #标记非垃圾邮件，1表示垃圾文件\n",
    "    vocabList = createVocabList(docList)  #创建词汇表，不重复\n",
    "    trainingSet = list(range(50))\n",
    "    testSet = []  #创建存储训练集的索引值的列表和测试集的索引值的列表\n",
    "    for i in range(10):  #从50个邮件中，随机挑选出40个作为训练集,10个做测试集\n",
    "        randIndex = int(np.random.uniform(0, len(trainingSet)))  #随机选取索索引值\n",
    "        testSet.append(trainingSet[randIndex])  #添加测试集的索引值\n",
    "        del (trainingSet[randIndex])  #在训练集列表中删除添加到测试集的索引值\n",
    "    trainMat = []\n",
    "    trainClasses = []  #创建训练集矩阵和训练集类别标签系向量\n",
    "    for docIndex in trainingSet:  #遍历训练集\n",
    "        trainMat.append(setOfWords2Vec(vocabList,\n",
    "                                       docList[docIndex]))  #将生成的词集模型添加到训练矩阵中\n",
    "        trainClasses.append(classList[docIndex])  #将类别添加到训练集类别标签系向量中\n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat),\n",
    "                               np.array(trainClasses))  #训练朴素贝叶斯模型\n",
    "    errorCount = 0  #错误分类计数\n",
    "    for docIndex in testSet:  #遍历测试集\n",
    "        wordVector = setOfWords2Vec(vocabList, docList[docIndex])  #测试集的词集模型\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V,\n",
    "                      pSpam) != classList[docIndex]:  #如果分类错误\n",
    "            errorCount += 1  #错误计数加1\n",
    "            print(\"分类错误的测试集：\", docList[docIndex])\n",
    "    print('错误率：%.2f%%' % (float(errorCount) / len(testSet) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f78dbe1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T09:33:44.790370Z",
     "start_time": "2023-09-28T09:33:44.752301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "错误率：0.00%\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c7d75",
   "metadata": {},
   "source": [
    "**效果不错！！！**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc4f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "339.865px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
